{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbF_Zw3KBazf"
      },
      "source": [
        "# **Demo on building data prep pipeline for model fine tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HU5bGauinf"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IBM/data-prep-kit/blob/dev/examples/notebooks/code/sample-notebook.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-NOkuTxiP7r",
        "outputId": "043f32fc-c476-433e-86b6-d7e9abd4d285"
      },
      "source": [
        "This demo notebook shows how to use [data-prep-kit](https://github.com/IBM/data-prep-kit) to build a data preparation pipeline that can be used for fine tuning or extended pre-training. We will discuss the various data preparation steps to process raw data (code repositories), tokenise it that can then be fine tuned using any popular code models. We will also discuss a novel recipe for semantic ordering of files in a repository which has shown to enhance model training. Please see our [paper](https://arxiv.org/abs/2407.13739) here for more details. For this demo, we will use the [codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code) dataset hosted on Hugging Face datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvqFDw4ruing"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install data-prep-toolkit and datasets library. This notebook requires atleast 8 cpus.\n",
        "To run on google colab, it is recommended to change the runtime to TPUs to get the required number of cpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PeyBOpAPuing"
      },
      "outputs": [],
      "source": [
        "%%capture logpip --no-stderr\n",
        "!pip3 install  'data-prep-toolkit[ray]==0.2.2.dev1'\n",
        "!pip3 install  'data-prep-toolkit-transforms[ray,all]==0.2.2.dev1'\n",
        "!pip install datasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VhIsZViaU2i"
      },
      "source": [
        "We use parallel processing capability using Ray, so that beyond the demo, a user can also use this for actual production runs on larger datasets, with minor code changes. Please read [here](https://github.com/IBM/data-prep-kit?tab=readme-ov-file#-about-) on various features of data-prep-kit that includes flexibility of compute to run from laptop to cluster.  There are three parameters, that the user can change, as per usecase:\n",
        "\n",
        "`runtime_num_worker`: number of parallel workers to be used\n",
        "\n",
        "`num_cpus`: number of cpus to be used per worker\n",
        "\n",
        "`run_locally: True` start a ray cluster for parallel computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J_UbnF9wbj95"
      },
      "outputs": [],
      "source": [
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "from data_processing.utils import ParamsUtils\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "#Default parameters for computation\n",
        "worker_options = {\"num_cpus\": 0.8}\n",
        "common_config_params = {\n",
        "        \"run_locally\": True,\n",
        "        \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "        \"runtime_num_workers\": 2,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8cwgKruinh"
      },
      "source": [
        "\n",
        "\n",
        "We will do all the processing in `sample_data` folder. This concludes our setup section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igofJZc4uinh"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data\n",
        "!mkdir -p sample_data\n",
        "!mkdir -p sample_data/hf_2_parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iIDB_6uinh"
      },
      "source": [
        "## Data Preparation Steps\n",
        "\n",
        "We now discuss the various data preparation steps to transform the raw data to a tokenised format post cleaning and transforming the data. We use the [parquet data format](https://parquet.apache.org/) for all our operations. This helps to efficiently scale the data for actual production runs, beyond the demo.\n",
        "\n",
        "1. HuggingFace2Parquet: Read the dataset from HF and convert into parquet format.\n",
        "2. Exact Deduplication: Remove exact duplicates.\n",
        "3. Fuzzy Deduplication: Remove near duplicates.\n",
        "4. Programming Lang Selection: Select the programming languages to be used for the analysis.\n",
        "5. Code Quality Annotations: Annotate whether a given code file is of high quality or not using various rules.\n",
        "6. Filtering: Filter dataset to retain only programming language of interest.\n",
        "7. Semantic Ordering: Organise code files by their semantic dependencies.  \n",
        "8. Tokenization: Tokenise the data for model fine tuning.\n",
        "\n",
        "The data processing pipeline is organised such that the output of the previous transform is used as input to the next one. Refer to the papers [here](https://arxiv.org/pdf/2405.04324) and [here](https://arxiv.org/abs/2407.13739) for complete details for each of the above steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xliMSdQEEwYx"
      },
      "source": [
        "## 1. Huggingface datasets to Parquet\n",
        "\n",
        "This is the first component of this pipeline. It ingests a dataset `codeparrot/github-code` from huggingface and converts it into\n",
        "parquet files for consumption by the next steps in this data processing pipeline.\n",
        "\n",
        "For this demo we are trying to process a few records. The following fields can be updated in case you want to use more data.\n",
        "_total_files_ = 10 <br/>\n",
        "_rows_per_file_ = 10\n",
        "\n",
        "The output of this stage of the pipeline would be written to `sample_data/hf_2_parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wit7ic1GauWN",
        "outputId": "039268a8-9212-4dd0-b0ba-76d5a714442e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sample_data/hf_2_parquet/data_0.parquet\n",
            "Writing sample_data/hf_2_parquet/data_1.parquet\n",
            "Writing sample_data/hf_2_parquet/data_2.parquet\n",
            "Writing sample_data/hf_2_parquet/data_3.parquet\n",
            "Writing sample_data/hf_2_parquet/data_4.parquet\n",
            "Writing sample_data/hf_2_parquet/data_5.parquet\n",
            "Writing sample_data/hf_2_parquet/data_6.parquet\n",
            "Writing sample_data/hf_2_parquet/data_7.parquet\n",
            "Writing sample_data/hf_2_parquet/data_8.parquet\n",
            "Writing sample_data/hf_2_parquet/data_9.parquet\n",
            "Writing sample_data/hf_2_parquet/data_10.parquet\n",
            "Writing sample_data/hf_2_parquet/data_11.parquet\n",
            "Writing sample_data/hf_2_parquet/data_12.parquet\n",
            "Writing sample_data/hf_2_parquet/data_13.parquet\n",
            "Writing sample_data/hf_2_parquet/data_14.parquet\n",
            "Writing sample_data/hf_2_parquet/data_15.parquet\n",
            "Writing sample_data/hf_2_parquet/data_16.parquet\n",
            "Writing sample_data/hf_2_parquet/data_17.parquet\n",
            "Writing sample_data/hf_2_parquet/data_18.parquet\n",
            "Writing sample_data/hf_2_parquet/data_19.parquet\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import uuid\n",
        "from data_processing.utils import TransformUtils\n",
        "from collections import defaultdict\n",
        "\n",
        "DATASET_NAME='codeparrot/github-code'\n",
        "\n",
        "ds = load_dataset(DATASET_NAME,\n",
        "                  streaming=True,\n",
        "                  split=\"train\",\n",
        "                  trust_remote_code=True)\n",
        "\n",
        "def row_mapper(row):\n",
        "    return {\n",
        "            'ext': TransformUtils.get_file_extension(row['path'])[1],\n",
        "            'document_id': str(uuid.uuid4())\n",
        "            }\n",
        "\n",
        "parquet_data_output = \"sample_data/hf_2_parquet\"\n",
        "\n",
        "## Converts a subset of a Hugging Face dataset to a Parquet file, optionally mapping and renaming columns.\n",
        "def hf_dataset_to_parquet(ds, skip, nrows, file_name, mapper=None, renamed_columns=[]):\n",
        "    dst_ = ds.skip(skip).take(nrows)\n",
        "\n",
        "    data_dict = defaultdict(list)\n",
        "\n",
        "    dst = dst_.map(mapper)\n",
        "\n",
        "    for data in dst:\n",
        "        for k, v in data.items():\n",
        "            data_dict[k].append(v)\n",
        "\n",
        "    for old, new in renamed_columns:\n",
        "        data_dict[new] = data_dict[old]\n",
        "        del data_dict[old]\n",
        "\n",
        "    table = pa.Table.from_pydict(data_dict)\n",
        "    pq.write_table(table, file_name)\n",
        "\n",
        "\n",
        "## Create parquet files\n",
        "\n",
        "total_files = 20\n",
        "rows_per_file = 20\n",
        "for num in range(total_files):\n",
        "    file_name = os.path.join(\n",
        "        f\"{parquet_data_output}\",\n",
        "        f\"data_{num}.parquet\"\n",
        "    )\n",
        "    print (f\"Writing {file_name}\")\n",
        "    hf_dataset_to_parquet(ds,\n",
        "                          1 * rows_per_file,\n",
        "                          rows_per_file,\n",
        "                          file_name=file_name,\n",
        "                          mapper=row_mapper,\n",
        "                          renamed_columns=[(\"code\", \"contents\"),\n",
        "                                           (\"path\", \"title\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N3Xqr0UAuinh",
        "outputId": "fd98bbc0-522e-41cb-c00b-c7ec8644d0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of rows, No of columns (400, 8)\n",
            "Sample data \n",
            " \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     repo_name language license   size  ext  \\\n",
              "0  decred/dcrd       GO     isc  10267  .go   \n",
              "\n",
              "                            document_id  \\\n",
              "0  7113625b-41ab-4ce0-bef3-89707f8d824a   \n",
              "\n",
              "                                            contents  \\\n",
              "0  // Copyright (c) 2021 The Decred developers\\n/...   \n",
              "\n",
              "                                    title  \n",
              "0  blockchain/indexers/indexsubscriber.go  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6de528f8-dd49-4f2b-9735-828d129dae98\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo_name</th>\n",
              "      <th>language</th>\n",
              "      <th>license</th>\n",
              "      <th>size</th>\n",
              "      <th>ext</th>\n",
              "      <th>document_id</th>\n",
              "      <th>contents</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>decred/dcrd</td>\n",
              "      <td>GO</td>\n",
              "      <td>isc</td>\n",
              "      <td>10267</td>\n",
              "      <td>.go</td>\n",
              "      <td>7113625b-41ab-4ce0-bef3-89707f8d824a</td>\n",
              "      <td>// Copyright (c) 2021 The Decred developers\\n/...</td>\n",
              "      <td>blockchain/indexers/indexsubscriber.go</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6de528f8-dd49-4f2b-9735-828d129dae98')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6de528f8-dd49-4f2b-9735-828d129dae98 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6de528f8-dd49-4f2b-9735-828d129dae98');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "input_df",
              "summary": "{\n  \"name\": \"input_df\",\n  \"rows\": 400,\n  \"fields\": [\n    {\n      \"column\": \"repo_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"decred/dcrd\",\n          \"bsander/dJSON\",\n          \"mpihlak/skytools-dev\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Python\",\n          \"Batchfile\",\n          \"Java\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"license\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"isc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9094,\n        \"min\": 152,\n        \"max\": 34797,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          10267\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ext\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \".py\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"d089ea3a-aeda-43d1-a704-ced003f639f9\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contents\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"// Copyright (c) 2021 The Decred developers\\n// Use of this source code is governed by an ISC\\n// license that can be found in the LICENSE file.\\n\\npackage indexers\\n\\nimport (\\n\\t\\\"context\\\"\\n\\t\\\"fmt\\\"\\n\\t\\\"sync\\\"\\n\\t\\\"sync/atomic\\\"\\n\\n\\t\\\"github.com/decred/dcrd/blockchain/v4/internal/progresslog\\\"\\n\\t\\\"github.com/decred/dcrd/database/v3\\\"\\n\\t\\\"github.com/decred/dcrd/dcrutil/v4\\\"\\n)\\n\\n// IndexNtfnType represents an index notification type.\\ntype IndexNtfnType int\\n\\nconst (\\n\\t// ConnectNtfn indicates the index notification signals a block\\n\\t// connected to the main chain.\\n\\tConnectNtfn IndexNtfnType = iota\\n\\n\\t// DisconnectNtfn indicates the index notification signals a block\\n\\t// disconnected from the main chain.\\n\\tDisconnectNtfn\\n)\\n\\nvar (\\n\\t// bufferSize represents the index notification buffer size.\\n\\tbufferSize = 128\\n\\n\\t// noPrereqs indicates no index prerequisites.\\n\\tnoPrereqs = \\\"none\\\"\\n)\\n\\n// IndexNtfn represents an index notification detailing a block connection\\n// or disconnection.\\ntype IndexNtfn struct {\\n\\tNtfnType          IndexNtfnType\\n\\tBlock             *dcrutil.Block\\n\\tParent            *dcrutil.Block\\n\\tPrevScripts       PrevScripter\\n\\tIsTreasuryEnabled bool\\n\\tDone              chan bool\\n}\\n\\n// IndexSubscription represents a subscription for index updates.\\ntype IndexSubscription struct {\\n\\tid         string\\n\\tidx        Indexer\\n\\tsubscriber *IndexSubscriber\\n\\tmtx        sync.Mutex\\n\\n\\t// prerequisite defines the notification processing hierarchy for this\\n\\t// subscription. It is expected that the subscriber associated with the\\n\\t// prerequisite provided processes notifications before they are\\n\\t// delivered by this subscription to its subscriber. An empty string\\n\\t// indicates the subscription has no prerequisite.\\n\\tprerequisite string\\n\\n\\t// dependent defines the index subscription that requires the subscriber\\n\\t// associated with this subscription to have processed incoming\\n\\t// notifications before it does. A nil dependency indicates the subscription\\n\\t// has no dependencies.\\n\\tdependent *IndexSubscription\\n}\\n\\n// newIndexSubscription initializes a new index subscription.\\nfunc newIndexSubscription(subber *IndexSubscriber, indexer Indexer, prereq string) *IndexSubscription {\\n\\treturn &IndexSubscription{\\n\\t\\tid:           indexer.Name(),\\n\\t\\tidx:          indexer,\\n\\t\\tprerequisite: prereq,\\n\\t\\tsubscriber:   subber,\\n\\t}\\n}\\n\\n// stop prevents any future index updates from being delivered and\\n// unsubscribes the associated subscription.\\nfunc (s *IndexSubscription) stop() error {\\n\\n\\t// If the subscription has a prerequisite, find it and remove the\\n\\t// subscription as a dependency.\\n\\tif s.prerequisite != noPrereqs {\\n\\t\\ts.mtx.Lock()\\n\\t\\tprereq, ok := s.subscriber.subscriptions[s.prerequisite]\\n\\t\\ts.mtx.Unlock()\\n\\t\\tif !ok {\\n\\t\\t\\treturn fmt.Errorf(\\\"no subscription found with id %s\\\", s.prerequisite)\\n\\t\\t}\\n\\n\\t\\tprereq.mtx.Lock()\\n\\t\\tprereq.dependent = nil\\n\\t\\tprereq.mtx.Unlock()\\n\\n\\t\\treturn nil\\n\\t}\\n\\n\\t// If the subscription has a dependent, stop it as well.\\n\\tif s.dependent != nil {\\n\\t\\terr := s.dependent.stop()\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t}\\n\\n\\t// If the subscription is independent, remove it from the\\n\\t// index subscriber's subscriptions.\\n\\ts.mtx.Lock()\\n\\tdelete(s.subscriber.subscriptions, s.id)\\n\\ts.mtx.Unlock()\\n\\n\\treturn nil\\n}\\n\\n// IndexSubscriber subscribes clients for index updates.\\ntype IndexSubscriber struct {\\n\\tsubscribers uint32 // update atomically.\\n\\n\\tc             chan IndexNtfn\\n\\tsubscriptions map[string]*IndexSubscription\\n\\tmtx           sync.Mutex\\n\\tctx           context.Context\\n\\tcancel        context.CancelFunc\\n\\tquit          chan struct{}\\n}\\n\\n// NewIndexSubscriber creates a new index subscriber. It also starts the\\n// handler for incoming index update subscriptions.\\nfunc NewIndexSubscriber(sCtx context.Context) *IndexSubscriber {\\n\\tctx, cancel := context.WithCancel(sCtx)\\n\\ts := &IndexSubscriber{\\n\\t\\tc:             make(chan IndexNtfn, bufferSize),\\n\\t\\tsubscriptions: make(map[string]*IndexSubscription),\\n\\t\\tctx:           ctx,\\n\\t\\tcancel:        cancel,\\n\\t\\tquit:          make(chan struct{}),\\n\\t}\\n\\treturn s\\n}\\n\\n// Subscribe subscribes an index for updates.  The returned index subscription\\n// has functions to retrieve a channel that produces a stream of index updates\\n// and to stop the stream when the caller no longer wishes to receive updates.\\nfunc (s *IndexSubscriber) Subscribe(index Indexer, prerequisite string) (*IndexSubscription, error) {\\n\\tsub := newIndexSubscription(s, index, prerequisite)\\n\\n\\t// If the subscription has a prequisite, find it and set the subscription\\n\\t// as a dependency.\\n\\tif prerequisite != noPrereqs {\\n\\t\\ts.mtx.Lock()\\n\\t\\tprereq, ok := s.subscriptions[prerequisite]\\n\\t\\ts.mtx.Unlock()\\n\\t\\tif !ok {\\n\\t\\t\\treturn nil, fmt.Errorf(\\\"no subscription found with id %s\\\", prerequisite)\\n\\t\\t}\\n\\n\\t\\tprereq.mtx.Lock()\\n\\t\\tdefer prereq.mtx.Unlock()\\n\\n\\t\\tif prereq.dependent != nil {\\n\\t\\t\\treturn nil, fmt.Errorf(\\\"%s already has a dependent set: %s\\\",\\n\\t\\t\\t\\tprereq.id, prereq.dependent.id)\\n\\t\\t}\\n\\n\\t\\tprereq.dependent = sub\\n\\t\\tatomic.AddUint32(&s.subscribers, 1)\\n\\n\\t\\treturn sub, nil\\n\\t}\\n\\n\\t// If the subscription does not have a prerequisite, add it to the index\\n\\t// subscriber's subscriptions.\\n\\ts.mtx.Lock()\\n\\ts.subscriptions[sub.id] = sub\\n\\ts.mtx.Unlock()\\n\\n\\tatomic.AddUint32(&s.subscribers, 1)\\n\\n\\treturn sub, nil\\n}\\n\\n// Notify relays an index notification to subscribed indexes for processing.\\nfunc (s *IndexSubscriber) Notify(ntfn *IndexNtfn) {\\n\\tsubscribers := atomic.LoadUint32(&s.subscribers)\\n\\n\\t// Only relay notifications when there are subscribed indexes\\n\\t// to be notified.\\n\\tif subscribers > 0 {\\n\\t\\tselect {\\n\\t\\tcase <-s.quit:\\n\\t\\tcase s.c <- *ntfn:\\n\\t\\t}\\n\\t}\\n}\\n\\n// findLowestIndexTipHeight determines the lowest index tip height among\\n// subscribed indexes and their dependencies.\\nfunc (s *IndexSubscriber) findLowestIndexTipHeight(queryer ChainQueryer) (int64, int64, error) {\\n\\t// Find the lowest tip height to catch up among subscribed indexes.\\n\\tbestHeight, _ := queryer.Best()\\n\\tlowestHeight := bestHeight\\n\\tfor _, sub := range s.subscriptions {\\n\\t\\ttipHeight, tipHash, err := sub.idx.Tip()\\n\\t\\tif err != nil {\\n\\t\\t\\treturn 0, bestHeight, err\\n\\t\\t}\\n\\n\\t\\t// Ensure the index tip is on the main chain.\\n\\t\\tif !queryer.MainChainHasBlock(tipHash) {\\n\\t\\t\\treturn 0, bestHeight, fmt.Errorf(\\\"%s: index tip (%s) is not on the \\\"+\\n\\t\\t\\t\\t\\\"main chain\\\", sub.idx.Name(), tipHash)\\n\\t\\t}\\n\\n\\t\\tif tipHeight < lowestHeight {\\n\\t\\t\\tlowestHeight = tipHeight\\n\\t\\t}\\n\\n\\t\\t// Update the lowest tip height if a dependent has a lower tip height.\\n\\t\\tdependent := sub.dependent\\n\\t\\tfor dependent != nil {\\n\\t\\t\\ttipHeight, _, err := sub.dependent.idx.Tip()\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\treturn 0, bestHeight, err\\n\\t\\t\\t}\\n\\n\\t\\t\\tif tipHeight < lowestHeight {\\n\\t\\t\\t\\tlowestHeight = tipHeight\\n\\t\\t\\t}\\n\\n\\t\\t\\tdependent = dependent.dependent\\n\\t\\t}\\n\\t}\\n\\n\\treturn lowestHeight, bestHeight, nil\\n}\\n\\n// CatchUp syncs all subscribed indexes to the the main chain by connecting\\n// blocks from after the lowest index tip to the current main chain tip.\\n//\\n// This should be called after all indexes have subscribed for updates.\\nfunc (s *IndexSubscriber) CatchUp(ctx context.Context, db database.DB, queryer ChainQueryer) error {\\n\\tlowestHeight, bestHeight, err := s.findLowestIndexTipHeight(queryer)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\t// Nothing to do if all indexes are synced.\\n\\tif bestHeight == lowestHeight {\\n\\t\\treturn nil\\n\\t}\\n\\n\\t// Create a progress logger for the indexing process below.\\n\\tprogressLogger := progresslog.NewBlockProgressLogger(\\\"Indexed\\\", log)\\n\\n\\t// tip and need to be caught up, so log the details and loop through\\n\\t// each block that needs to be indexed.\\n\\tlog.Infof(\\\"Catching up from height %d to %d\\\", lowestHeight,\\n\\t\\tbestHeight)\\n\\n\\tvar cachedParent *dcrutil.Block\\n\\tfor height := lowestHeight + 1; height <= bestHeight; height++ {\\n\\t\\tif interruptRequested(ctx) {\\n\\t\\t\\treturn indexerError(ErrInterruptRequested, interruptMsg)\\n\\t\\t}\\n\\n\\t\\thash, err := queryer.BlockHashByHeight(height)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\t// Ensure the next tip hash is on the main chain.\\n\\t\\tif !queryer.MainChainHasBlock(hash) {\\n\\t\\t\\tmsg := fmt.Sprintf(\\\"the next block being synced to (%s) \\\"+\\n\\t\\t\\t\\t\\\"at height %d is not on the main chain\\\", hash, height)\\n\\t\\t\\treturn indexerError(ErrBlockNotOnMainChain, msg)\\n\\t\\t}\\n\\n\\t\\tvar parent *dcrutil.Block\\n\\t\\tif cachedParent == nil && height > 0 {\\n\\t\\t\\tparentHash, err := queryer.BlockHashByHeight(height - 1)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\treturn err\\n\\t\\t\\t}\\n\\t\\t\\tparent, err = queryer.BlockByHash(parentHash)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\treturn err\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tparent = cachedParent\\n\\t\\t}\\n\\n\\t\\tchild, err := queryer.BlockByHash(hash)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\t// Construct and send the index notification.\\n\\t\\tvar prevScripts PrevScripter\\n\\t\\terr = db.View(func(dbTx database.Tx) error {\\n\\t\\t\\tif interruptRequested(ctx) {\\n\\t\\t\\t\\treturn indexerError(ErrInterruptRequested, interruptMsg)\\n\\t\\t\\t}\\n\\n\\t\\t\\tprevScripts, err = queryer.PrevScripts(dbTx, child)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\treturn err\\n\\t\\t\\t}\\n\\n\\t\\t\\treturn nil\\n\\t\\t})\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\tisTreasuryEnabled, err := queryer.IsTreasuryAgendaActive(parent.Hash())\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\tntfn := &IndexNtfn{\\n\\t\\t\\tNtfnType:          ConnectNtfn,\\n\\t\\t\\tBlock:             child,\\n\\t\\t\\tParent:            parent,\\n\\t\\t\\tPrevScripts:       prevScripts,\\n\\t\\t\\tIsTreasuryEnabled: isTreasuryEnabled,\\n\\t\\t}\\n\\n\\t\\t// Relay the index update to subscribed indexes.\\n\\t\\tfor _, sub := range s.subscriptions {\\n\\t\\t\\terr := updateIndex(ctx, sub.idx, ntfn)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\ts.cancel()\\n\\t\\t\\t\\treturn err\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tcachedParent = child\\n\\n\\t\\tprogressLogger.LogBlockHeight(child.MsgBlock(), parent.MsgBlock())\\n\\t}\\n\\n\\tlog.Infof(\\\"Caught up to height %d\\\", bestHeight)\\n\\n\\treturn nil\\n}\\n\\n// Run relays index notifications to subscribed indexes.\\n//\\n// This should be run as a goroutine.\\nfunc (s *IndexSubscriber) Run(ctx context.Context) {\\n\\tfor {\\n\\t\\tselect {\\n\\t\\tcase ntfn := <-s.c:\\n\\t\\t\\t// Relay the index update to subscribed indexes.\\n\\t\\t\\tfor _, sub := range s.subscriptions {\\n\\t\\t\\t\\terr := updateIndex(ctx, sub.idx, &ntfn)\\n\\t\\t\\t\\tif err != nil {\\n\\t\\t\\t\\t\\tlog.Error(err)\\n\\t\\t\\t\\t\\ts.cancel()\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\tif ntfn.Done != nil {\\n\\t\\t\\t\\tclose(ntfn.Done)\\n\\t\\t\\t}\\n\\n\\t\\tcase <-ctx.Done():\\n\\t\\t\\tlog.Infof(\\\"Index subscriber shutting down\\\")\\n\\n\\t\\t\\tclose(s.quit)\\n\\n\\t\\t\\t// Stop all updates to subscribed indexes and terminate their\\n\\t\\t\\t// processes.\\n\\t\\t\\tfor _, sub := range s.subscriptions {\\n\\t\\t\\t\\terr := sub.stop()\\n\\t\\t\\t\\tif err != nil {\\n\\t\\t\\t\\t\\tlog.Error(\\\"unable to stop index subscription: %v\\\", err)\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\ts.cancel()\\n\\t\\t\\treturn\\n\\t\\t}\\n\\t}\\n}\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"blockchain/indexers/indexsubscriber.go\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#Function to read parquet files in a directory as pandas dataframe\n",
        "from pathlib import Path\n",
        "def read_parquet_bulk(dir_path):\n",
        "    data_dir = Path(dir_path)\n",
        "    # Get the list of all Parquet files in the directory\n",
        "    parquet_files = list(data_dir.glob('*.parquet'))\n",
        "    # Check if the directory contains any Parquet files\n",
        "    if not parquet_files:\n",
        "        raise ValueError(f\"No Parquet files found in directory: {dir_path}\")\n",
        "    # Concatenate all Parquet files into a single DataFrame\n",
        "    full_df = pd.concat(\n",
        "        pd.read_parquet(parquet_file)\n",
        "        for parquet_file in parquet_files\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    return full_df\n",
        "\n",
        "\n",
        "input_df=read_parquet_bulk(parquet_data_output)\n",
        "\n",
        "print(\"No of rows, No of columns\",input_df.shape)\n",
        "print(\"Sample data \\n \")\n",
        "input_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTTGvLBuinh"
      },
      "source": [
        "## 2. Exact deduplication\n",
        "\n",
        "This step will find exact duplicates in the 'content' column and remove them. This is done by computing SHA256 hash on the code files and remove records having identical hashes.\n",
        "\n",
        "The transform specific params for exact deduplication are: <br/>\n",
        " _ededup_hash_cpu_ -  Number of cpus per worker <br/>\n",
        " _ededup_num_hashes_ - Number of workers used to store hashes <br/>\n",
        " _ededup_doc_column_ - Name of column which has to be checked for deduplication <br/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRUfjHExbd1g",
        "outputId": "ca1dec5b-d910-414e-cbab-c02fac46317e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "01:26:45 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
            "INFO:ededup_transform_base:exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
            "01:26:45 INFO - pipeline id pipeline_id\n",
            "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
            "01:26:45 INFO - code location None\n",
            "INFO:data_processing.runtime.execution_configuration:code location None\n",
            "01:26:45 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
            "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
            "01:26:45 INFO - actor creation delay 0\n",
            "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
            "01:26:45 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
            "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
            "01:26:45 INFO - data factory data_ is using local data access: input_folder - sample_data/hf_2_parquet output_folder - sample_data/ededup_out\n",
            "INFO:data_processing.data_access.data_access_factory_base038a25fc-dc83-4ef8-97f1-e31d74b9ecd4:data factory data_ is using local data access: input_folder - sample_data/hf_2_parquet output_folder - sample_data/ededup_out\n",
            "01:26:45 INFO - data factory data_ max_files -1, n_sample -1\n",
            "INFO:data_processing.data_access.data_access_factory_base038a25fc-dc83-4ef8-97f1-e31d74b9ecd4:data factory data_ max_files -1, n_sample -1\n",
            "01:26:45 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
            "INFO:data_processing.data_access.data_access_factory_base038a25fc-dc83-4ef8-97f1-e31d74b9ecd4:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
            "01:26:45 INFO - Running locally\n",
            "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n",
            "2024-10-28 01:26:46,751\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "01:26:48 INFO - Exception running ray remote orchestration\n",
            "\u001b[36mray::orchestrate()\u001b[39m (pid=18052, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/data_processing_ray/runtime/ray/transform_orchestrator.py\", line 57, in orchestrate\n",
            "    runtime = runtime_config.create_transform_runtime()\n",
            "AttributeError: 'EdedupRayTransformConfiguration' object has no attribute 'create_transform_runtime'\n",
            "INFO:data_processing_ray.runtime.ray.transform_launcher:Exception running ray remote orchestration\n",
            "\u001b[36mray::orchestrate()\u001b[39m (pid=18052, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/data_processing_ray/runtime/ray/transform_orchestrator.py\", line 57, in orchestrate\n",
            "    runtime = runtime_config.create_transform_runtime()\n",
            "AttributeError: 'EdedupRayTransformConfiguration' object has no attribute 'create_transform_runtime'\n",
            "01:26:48 INFO - Completed execution in 0.056 min, execution result 1\n",
            "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 0.056 min, execution result 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from ededup_transform_ray import EdedupRayTransformRuntimeConfiguration\n",
        "\n",
        "input_folder = parquet_data_output # Output of previous stage is used as input.\n",
        "output_folder = \"sample_data/ededup_out\"\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "\n",
        "ededup_params = {\n",
        "    # ededup parameters\n",
        "    \"ededup_hash_cpu\": 0.5,\n",
        "    \"ededup_num_hashes\": 2,\n",
        "    \"ededup_doc_column\": \"contents\",\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "params = common_config_params | ededup_params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "ededup_launcher = RayTransformLauncher(EdedupRayTransformRuntimeConfiguration())\n",
        "ededup_launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bue76TYvuini"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pprint\n",
        "def read_metadata(path):\n",
        "    with open(path, 'r') as file:\n",
        "        metadata = json.load(file)\n",
        "        pprint.pp(metadata)\n",
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmvmoTMauini"
      },
      "source": [
        "## 3. Fuzzy Deduplication\n",
        "\n",
        "This step will find near duplicates and remove them. The code is broken into two code cells, one for adding document ids to the parquet file and then running fuzzy dedup. Document id addition is a prerequisite for fuzzy dedup.\n",
        "\n",
        "We first add the document ids as an additional column to the parquet files. <br/>\n",
        "_doc_column_ - specifies name of the column containing the document (required for ID generation) <br/>\n",
        "_hash_column_ - specifies name of the column created to hold the string document id, if None, id is not generated <br/>\n",
        "_int_id_column_ - specifies name of the column created to hold the integer document id, if None, id is not generated <br/>\n",
        "At least one of hash_column or int_id_column must be specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4cYttNlbgf0"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/ededup_out\"\n",
        "output_folder = \"sample_data/docid_out\"\n",
        "\n",
        "\n",
        "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "\n",
        "doc_id_params = {\n",
        "    # doc id configuration\n",
        "    \"doc_id_doc_column\": \"contents\",\n",
        "    \"doc_id_hash_column\": \"hash_column\",\n",
        "    \"doc_id_int_column\": \"int_id_column\",\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "params = doc_id_params | common_config_params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
        "launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mxs9lwCuini"
      },
      "outputs": [],
      "source": [
        "input_df=read_parquet_bulk(output_folder)\n",
        "input_df.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5WiJt0Yuini"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCpgXRrkuini"
      },
      "source": [
        "Post adding the document ids, the next step is to run fuzzy deduplication. We apply a two-step method for this: (1) compute MinHashes of all the documents and then utilize Locally Sensitive Hashing (LSH) to group documents based on their MinHash fingerprints, (2) measure Jaccard similarity between each pair of documents\n",
        "in the same bucket and annotate documents except one as duplicates based on a similarity\n",
        "threshold.  \n",
        "\n",
        "Some important transform specific params are: <br/>\n",
        "_fdedup_doc_column_ - Column to be used for deduplication <br/>\n",
        "_fdedup_threshold_ - specifies the Jaccard similarity threshold (default is 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b11MMQEheO6q"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/docid_out\"\n",
        "output_folder = \"sample_data/fdedup_out\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from data_processing.utils import ParamsUtils\n",
        "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\": 0.8}\n",
        "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
        "fdedup_params = {\n",
        "    # columns used\n",
        "    \"fdedup_doc_column\": \"contents\",\n",
        "    \"fdedup_id_column\": \"int_id_column\",\n",
        "    \"fdedup_cluster_column\": \"hash_column\",\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "params = common_config_params| fdedup_params\n",
        "\n",
        "# Pass commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# launch\n",
        "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
        "fdedup_launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-GCzYyauini"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B06Nzfvkuini"
      },
      "source": [
        "## 4. Programming Language Selection\n",
        "\n",
        "This module helps retain the code files for language of interest which can be specified using selected_languages_file. Post this step, a new column is added, that contains the programming language name. One can use the code in the Filtering step to do analytics on how many files are found for which languages and thereby selectively filter.\n",
        "\n",
        "The important parameters used by this transform are: <br/>\n",
        "_lang_allowed_langs_file_key_ - A file with a list of allowed languages. <br/>\n",
        "_lang_lang_column_key_ - The name of column which has programming language. <br/>\n",
        "_lang_output_column_key_ - The name of annotation column. <br/>\n",
        "\n",
        "For this demo, we will use this [file](https://github.com/IBM/data-prep-kit/blob/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt) to specify languages of interest and the module will add a new column called \"language_of_interest\" which can have two values 0/1. 1 is added for all rows that have code files belonging to programming language specified in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGaG8NWUAbAu"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/fdedup_out\"\n",
        "output_folder = \"sample_data/ps_out\"\n",
        "\n",
        "# download allowed-code-languages.txt\n",
        "# !wget https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt\n",
        "\n",
        "# Create a file with language of interest\n",
        "! echo \"JavaScript\\nC++\\nC\\nGo\\nJava\" >> allowed-code-languages.txt\n",
        "\n",
        "selected_languages_file = \"./allowed-code-languages.txt\"\n",
        "\n",
        "from proglang_select_transform_ray import ProgLangSelectRayConfiguration\n",
        "from proglang_select_transform import (\n",
        "    lang_allowed_langs_file_key,\n",
        "    lang_lang_column_key,\n",
        "    lang_output_column_key,\n",
        ")\n",
        "\n",
        "# create parameters\n",
        "language_column_name = \"language\"\n",
        "annotated_column_name = \"language_of_interest\"\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "\n",
        "langselect_config = {\n",
        "    lang_allowed_langs_file_key: selected_languages_file,\n",
        "    lang_lang_column_key: language_column_name,\n",
        "    lang_output_column_key: annotated_column_name,\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "params = common_config_params| langselect_config\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
        "launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh6wuCunuini"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHFuZee1uini"
      },
      "outputs": [],
      "source": [
        "read_parquet_bulk(output_folder).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exauX7o2uini"
      },
      "source": [
        "## 5. Code Quality\n",
        "\n",
        "We experiment with various code quality metrics but finally retain the four code quality metrics used by (Li et al., 2023) to balance the tradeoff between code quality versus data volume.\n",
        "\n",
        "Quality metrics\n",
        "\n",
        "'line_mean': Average of the total line lengths.\n",
        "'line_max':  Maximum line length present .\n",
        "'total_num_lines': Total number of lines present\n",
        "'avg_longest_lines': Average of the first n longest lines, where n can be any number you choose.\n",
        "'alphanum_frac':  Calculates average of alpha numeric with respect to total data\n",
        "'char_token_ratio': Computes character/token ratio of the file with tokenizer\n",
        "'autogenerated': Check if file is autogenerated by looking for keywords in the first few lines of the file.\n",
        "'config_or_test':  Check if file is a configuration file or a unit test\n",
        "'has_no_keywords': Check if a python file has none of the keywords - for funcion, class, for loop, while loop.\n",
        "'has_few_assignments': Check if file uses symbol '=' less than 'minimum' times\n",
        "'is_xml': Check if input data is xml content\n",
        "'is_html': Check if input data is HTML files based on displayed text VS code ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lK4qf-euini"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/ps_out\"\n",
        "output_folder = \"sample_data/cq_out\"\n",
        "\n",
        "from code_quality_transform_ray import CodeQualityRayTransformConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "# ??\n",
        "\n",
        "\n",
        "language_column_name = \"language\"\n",
        "params = {\n",
        "    \"cq_contents_column_name\": \"contents\",\n",
        "    \"cq_language_column_name\": language_column_name,\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "params = common_config_params| params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(CodeQualityRayTransformConfiguration())\n",
        "# launch\n",
        "launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T55t7FyBuini"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viEACibeuini"
      },
      "outputs": [],
      "source": [
        "read_parquet_bulk(output_folder).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXu_i9jLAo9H"
      },
      "source": [
        "## 6. Filtering\n",
        "\n",
        "This step can be used to filter the code files based on our chosen conditions. In this demo example, we have only used one annotation of adding programming language names for each code file. To demonstrate the utility, we will use this module to retain only code files of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAl7B58oAyZQ"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/cq_out\"\n",
        "output_folder = \"sample_data/filter_out\"\n",
        "\n",
        "\n",
        "from filter_transform import (\n",
        "    filter_columns_to_drop_cli_param,\n",
        "    filter_criteria_cli_param,\n",
        "    filter_logical_operator_cli_param,\n",
        ")\n",
        "from filter_transform_ray import FilterRayTransformConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "\n",
        "# This is just an example criteria to filter\n",
        "filter_criteria = [\n",
        "    \"language_of_interest = 1\",\n",
        "    \"total_num_lines > 10 AND total_num_lines < 90\"\n",
        "]\n",
        "filter_logical_operator = \"AND\"\n",
        "filter_columns_to_drop = [\"language_of_interest\", \"hash_column\"]\n",
        "\n",
        "filter_params = {\n",
        "    filter_criteria_cli_param: filter_criteria,\n",
        "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
        "    filter_logical_operator_cli_param: filter_logical_operator,\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(common_config_params| filter_params)\n",
        "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
        "launcher.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rOhh0T3uinj"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QisaEA-3uinj"
      },
      "source": [
        "## 7. Semantic Ordering of Code Files\n",
        "\n",
        "In this step, we order the code files such that we pack files from the same repository together, arranging them to prioritize semantic dependencies. We identify these dependencies by analyzing file imports and create a directed acyclic graph, where each file is a node and edges represent API imports between files. After breaking any cycles in the graph, we perform a topological sort to establish an ordering of files based on their semantic dependencies. We then organize the files in a repository by placing documentation and build files first, followed by the ordered set of files with semantic dependencies, and finally the remaining non-connected files. These non-connected files are arranged according to their folder structure, using a depth-first search to traverse the repository. Finally, we determine the dominant programming language of a repository based on file extensions and presence of build files, to organise repo-ordered files by programming languages.\n",
        "\n",
        "\n",
        "This transform has following parameters:  <br/>\n",
        " _repo_lvl_sorting_enabled_ - If True, the repo level output is sorted using _repo_lvl_sorting_algo_ <br/>\n",
        " _repo_lvl_sorting_algo_ - Select the sorting algorithm to be used for repo level sorting. Use SORT_SEMANTIC_NORMALISED to organise by semantic dependencies or SORT_BY_PATH to arrange files based on folder structure in a repository.  <br/>\n",
        " _repo_lvl_store_backend_dir_ -  Directory to use for local store. Needed only when repo_lvl_store_type=local <br/>\n",
        " _repo_lvl_output_by_langs_ - If True, it organises output into folders of programming language. <br/>\n",
        " _repo_lvl_combine_rows_ - If True, it combines the contents of repo into a single row. <br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoYuulO4uinj"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/filter_out\"\n",
        "output_folder = \"sample_data/rlo_out\"\n",
        "\n",
        "import tempfile\n",
        "from repo_level_order_transform import RepoLevelOrderRayTransformConfiguration\n",
        "with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "\n",
        "    # create parameters\n",
        "    local_conf = {\n",
        "        \"input_folder\": input_folder,\n",
        "        \"output_folder\": output_folder,\n",
        "     }\n",
        "\n",
        "    worker_options = {\"num_cpus\": 0.8}\n",
        "    code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
        "\n",
        "    repo_level_params = {\n",
        "        \"repo_lvl_sorting_algo\": \"SORT_SEMANTIC_NORMALISED\",\n",
        "        \"repo_lvl_store_type\": \"local\",\n",
        "        \"repo_lvl_store_backend_dir\": tmpdirname,\n",
        "        \"repo_lvl_output_by_langs\": True,\n",
        "        \"repo_lvl_combine_rows\": True,\n",
        "        \"repo_lvl_sorting_enabled\": True,\n",
        "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "    }\n",
        "\n",
        "\n",
        "    sys.argv = ParamsUtils.dict_to_req(d= common_config_params| repo_level_params)\n",
        "    launcher = RayTransformLauncher(RepoLevelOrderRayTransformConfiguration())\n",
        "    launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90TiUWjAuinj"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byK75Kb1A3E7"
      },
      "source": [
        "## 8. Tokenization\n",
        "\n",
        "Next, we tokenize the data to be used for fine tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBYg93WMBBq6"
      },
      "outputs": [],
      "source": [
        "input_folder = \"sample_data/rlo_out\"\n",
        "output_folder = \"sample_data/tokenize_out\"\n",
        "\n",
        "from tokenization_transform_ray import TokenizationRayConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "\n",
        "tf_params= {\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
        "}\n",
        "sys.argv = ParamsUtils.dict_to_req(d=common_config_params| tf_params)\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
        "# Launch the ray actor(s) to process the input\n",
        "launcher.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC2OsFrxuinm"
      },
      "outputs": [],
      "source": [
        "read_metadata(f\"{output_folder}/metadata.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BAfFcVQuinm"
      },
      "outputs": [],
      "source": [
        "read_parquet_bulk(f\"{output_folder}/C\").head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFUrzzjeBFfJ"
      },
      "source": [
        "**The data is now ready for extended pretraining or fine tuning using any open source code models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLz41tL3uinm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}